# -*- coding: utf-8 -*-
"""Drippy AI VIT AP CSI Workshop.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XyYH7p-qaqW4EmGdalvyMoDG_AswdMIZ
"""

!pip install langchain langchain-google-genai langchain_core diffusers numpy torch transformers xformers accelerate controlnet_aux

from diffusers.utils import load_image
from controlnet_aux import OpenposeDetector
import urllib.request
from io import BytesIO
import cv2
from PIL import Image
import numpy as np
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
import torch
from diffusers import UniPCMultistepScheduler



def image_grid(imgs, rows, cols):
    assert len(imgs) == rows * cols

    w, h = imgs[0].size
    grid = Image.new("RGB", size=(cols * w, rows * h))
    grid_w, grid_h = grid.size

    for i, img in enumerate(imgs):
        grid.paste(img, box=(i % cols * w, i // cols * h))
    return grid



urllib.request.urlretrieve(
  'https://media.istockphoto.com/id/965662288/photo/a-young-bearded-man-in-casual-wear-stands-with-hands-on-his-sides-on-a-white-background.jpg?s=612x612&w=0&k=20&c=QNRZO_MJP-hANRVdtyJ3OeVx30JM5IDrCjPwLbw8NjQ=',
  "reference.jpg")

image = Image.open("reference.jpg")
ogImage = image
ogImage

image = np.array(image)
low_threshold = 100
high_threshold = 200

image = cv2.Canny(image, low_threshold, high_threshold)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)
canny_image

controlnet = ControlNetModel.from_pretrained("fusing/stable-diffusion-v1-5-controlnet-openpose", torch_dtype=torch.float32)
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float32
)
pipe = pipe.to("cpu")
model = OpenposeDetector.from_pretrained("lllyasviel/ControlNet")
pose = model(ogImage)
pose

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.output_parsers import PydanticOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field



if not api_key:
    print('Google API key not found in environment variables.')
    exit()

llm = ChatGoogleGenerativeAI(model="gemini-pro", api_key=api_key)

class OutfitModel(BaseModel):
    list_of_items: list[str] = Field(
        description="List of INDIVIDUAL CLOTHING ITEMS AND ACCESSORIES")

def generate_outfit_list(outfit):
    parser = PydanticOutputParser(pydantic_object=OutfitModel)
    prompt = PromptTemplate(
        template="You are an expert at forming a list of INDIVIDUAL CLOTHING ITEMS AND ACCESSORIES"
                 "Each item in the list should be searchable in an e commerce website so generate the results accordingly."
                 "FORM A LIST OF INDIVIDUAL CLOTHING ITEMS AND ACCESSORIES FOR THE GIVEN INPUT WITH DESCRIPTION"
                 "Each item in the list should be descriptive and not depend on desciption of other items."
                 "Do not give multiple options to the user. Do not include the word 'or' or any equivalent to it."
                 "\n{format_instructions}, \nInput: {input}",
        input_variables=["input"],
        partial_variables={"format_instructions": parser.get_format_instructions()})
    chain = prompt | llm | parser
    ai_response = chain.invoke({"input": outfit})
    ai_response = ai_response.list_of_items
    return ai_response

def search(outfit):
    ai_response = generate_outfit_list(outfit)
    links = []
    for i in ai_response:
        t = i.replace(" ", "%20")
        l = "https://www.flipkart.com/search?marketplace=FLIPKART&q=" + t
        links.append(l)
        print(i, l)

def generate_response(conversation_history, human_input):
    prompt = PromptTemplate(
        template="You are Drippy AI. You are a helpful assistant that recommends users OUTFIT with ACCESSORIES based upon their personalised fashion needs, gender, age, location and occasion. IF THE USER ASKS ANYTHING ELSE DENY THEM."
                 "Ask open ended questions to determine the users gender,age,location. Listen carefully to the response and take notes."
                 "Do not Include Purchase Links AND Price"
                 "GENERATE AN OUTFIT ONLY AFTER COLLECTING AGE GENDER AND LOCATION ,ONCE COMPLETED RETURN THE OUTFIT AND SPECIFY FOR WHOM THE USER'S AGE, GENDER AND LOCATION."
                 "Do not give multiple options to the user. Do not include the word 'or' or any equivalent to it."
                 "\nConversation History = {conversation_history},\nHuman Input: {human_input}",
        input_variables=["human_input", "conversation_history"],
    )
    chain = prompt | llm
    ai_response = chain.invoke({"human_input": human_input, "conversation_history": conversation_history})
    ai_response = ai_response.content
    return ai_response

def generate_image(conversation_history):
    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)

    pipe.enable_model_cpu_offload()

    pipe.enable_xformers_memory_efficient_attention()
    prompt = PromptTemplate(
        template="Generate a comprehensive prompt that is 1 para long and at the maximum 3 sentences it should less than 70 tokens to generate image of the outfit from the conversation history."
                 "Include description of the human and the outfit in detail"
                 "\Coversation Histroy: {conversation_history}",
        input_variables=["outfit"],
    )
    chain = prompt | llm
    ai_response = chain.invoke({"conversation_history": conversation_history})
    ai_response = ai_response.content
    print(ai_response)
    prompt = ai_response

    generator = torch.Generator().manual_seed(2)

    output = pipe(
        prompt,
        pose,
        negative_prompt="monochrome, lowres, bad anatomy, worst quality, low quality",
        num_inference_steps=20,
        generator=generator,
    )

    outputImage = output.images[0]
    outputImage
    display(outputImage)

conversation_history = []

while True:
    human_input = input("You: ")
    try:
        if human_input == "Search":
            search(conversation_history[-1]["AI"])
        elif human_input == "Show Image":
            generate_image(conversation_history)
        elif human_input == "Exit":
            exit()
        else:
            ai_response = generate_response(conversation_history, human_input)
            conversation_history.append({"Human": human_input, "AI": ai_response})
            print("AI:", ai_response)
    except Exception as e:
        print("An error occoured please try again!!", str(e))